{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70de5895-00e1-4aa2-97d3-827ee91bf435",
   "metadata": {},
   "source": [
    "# BIG DATA PROJECT - HADOOP HEROES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c84e6d5-1da8-43e3-a344-3ca6a8c9440a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.conf import SparkConf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "35fa0a2f-cf73-47d3-9fb2-05c8cdca56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23/11/09 23:10:09 WARN Utils: Your hostname, localhost.localdomain resolves to a loopback address: 127.0.0.1; using 10.21.10.196 instead (on interface eth0)\n",
      "23/11/09 23:10:09 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/11/09 23:10:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark Session WebUI Port: 4040\n"
     ]
    }
   ],
   "source": [
    "from os.path import abspath\n",
    "warehouse_location = abspath('spark-warehouse')\n",
    "spark = SparkSession.builder \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .appName(\"ISM6562 Spark Project\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "\n",
    "# Let's get the SparkContext object. It's the entry point to the Spark API. It's created when you create a sparksession\n",
    "sc = spark.sparkContext  \n",
    "sc.setLogLevel(\"ERROR\") # only display errors (not warnings)\n",
    "\n",
    "# note: If you have multiple spark sessions running (like from a previous notebook you've run), \n",
    "# this spark session webUI will be on a different port than the default (4040). One way to \n",
    "# identify this part is with the following line. If there was only one spark session running, \n",
    "# this will be 4040. If it's higher, it means there are still other spark sesssions still running.\n",
    "spark_session_port = spark.sparkContext.uiWebUrl.split(\":\")[-1]\n",
    "print(\"Spark Session WebUI Port: \" + spark_session_port)\n",
    "\n",
    "# It's best if you find that the port number displayed below is not 4040, then you should shut down all other spark sessions and \n",
    "# run this code again. If you don't, you may have trouble accessing the data in the spark-warehouse directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b8f58cef-4d8c-4b74-8713-d63de7a24b06",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - hive</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://linux:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.5.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>ISM6562 Spark Project</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f741c5f2680>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b30b8e0a-17f3-4575-a10c-48a5af70cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import data from csv file"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3dfe4655-c2ae-444b-8964-ec24ec62a1ac",
   "metadata": {},
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "raw",
   "id": "eda1271c-60c6-4a28-9de8-a7f8a257d70f",
   "metadata": {},
   "source": [
    "df = pd.read_csv('data/yellow_tripdata_2022-02.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2f7e98-817d-4527-90ba-5fb97a5e0d30",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f4d0cd-80d4-4675-a08d-68c4014cd01f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8bff1a2d-2a80-4108-8265-d7f48fd1b335",
   "metadata": {},
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "32e5c4af-4e2d-49c6-90b5-ae5fe20a6d65",
   "metadata": {},
   "source": [
    "df=spark.sql(\"show databases\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4cf18266-eceb-4453-962c-a142735ab653",
   "metadata": {},
   "source": [
    "tables = spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "595d6b06-6b60-4c3d-bf22-caf3f55689d9",
   "metadata": {},
   "source": [
    "#Load data to warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1ac0be0c-7a49-46a7-a5da-86f73012d810",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|       1|    02-01-2022 00:06|     02-01-2022 00:19|              1|          5.4|         138|         252|       17.0| 1.75|       3.9|       23.45|       1.25|\n",
      "|       1|    02-01-2022 00:38|     02-01-2022 00:55|              1|          6.4|         138|          41|       21.0| 1.75|       0.0|        30.1|       1.25|\n",
      "|       1|    02-01-2022 00:03|     02-01-2022 00:26|              1|         12.5|         138|         200|       35.5| 1.75|       0.0|        44.6|       1.25|\n",
      "|       2|    02-01-2022 00:08|     02-01-2022 00:28|              1|         9.88|         239|         200|       28.0|  0.5|       0.0|        34.8|        0.0|\n",
      "|       2|    02-01-2022 00:06|     02-01-2022 00:33|              1|        12.16|         138|         125|       35.5|  0.5|      8.11|       48.66|       1.25|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip = spark.read.csv('data/yellow_tripdata_2022-02.csv', header=True, inferSchema=True);\n",
    "\n",
    "# display the first 5 rows of the dataframe\n",
    "trip.show(5);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94b17aeb-dad7-423d-8793-fad5377b8852",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- VendorID: integer (nullable = true)\n",
      " |-- tpep_pickup_datetime: string (nullable = true)\n",
      " |-- tpep_dropoff_datetime: string (nullable = true)\n",
      " |-- passenger_count: integer (nullable = true)\n",
      " |-- trip_distance: double (nullable = true)\n",
      " |-- PULocationID: integer (nullable = true)\n",
      " |-- DOLocationID: integer (nullable = true)\n",
      " |-- fare_amount: double (nullable = true)\n",
      " |-- extra: double (nullable = true)\n",
      " |-- tip_amount: double (nullable = true)\n",
      " |-- total_amount: double (nullable = true)\n",
      " |-- airport_fee: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ea7bdf79-9ce1-46a1-8d5f-8001cc53e4cb",
   "metadata": {},
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "# Assuming your DataFrame is named 'df', and you want to create a new column 'TIP_STATUS'\n",
    "trip = trip.withColumn(\"TIP_STATUS\", F.when(trip[\"tip_amount\"] > 0, 1).otherwise(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "170c6679-8c68-4b2f-9056-7449602cc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the DB and table in spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fbc49216-16ae-4998-aa49-8dcf36a7f9c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "trip.createOrReplaceTempView(\"trip_tmp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "15131916-735d-4fc5-9c08-53d371fbacad",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|       1|    02-01-2022 00:06|     02-01-2022 00:19|              1|          5.4|         138|         252|       17.0| 1.75|       3.9|       23.45|       1.25|\n",
      "|       1|    02-01-2022 00:38|     02-01-2022 00:55|              1|          6.4|         138|          41|       21.0| 1.75|       0.0|        30.1|       1.25|\n",
      "|       1|    02-01-2022 00:03|     02-01-2022 00:26|              1|         12.5|         138|         200|       35.5| 1.75|       0.0|        44.6|       1.25|\n",
      "|       2|    02-01-2022 00:08|     02-01-2022 00:28|              1|         9.88|         239|         200|       28.0|  0.5|       0.0|        34.8|        0.0|\n",
      "|       2|    02-01-2022 00:06|     02-01-2022 00:33|              1|        12.16|         138|         125|       35.5|  0.5|      8.11|       48.66|       1.25|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM trip_tmp_view\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a0410bbd-8ca2-4ee5-9bc2-bb5fcb491690",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a98dd2cb-4aba-4d68-93a9-b296e245d462",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------------+-----------+\n",
      "|namespace|    tableName|isTemporary|\n",
      "+---------+-------------+-----------+\n",
      "|         |trip_tmp_view|       true|\n",
      "+---------+-------------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tables = spark.sql(\"show tables\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f20392b7-0843-4b65-bf78-4bddee7952b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save table in spark data warehouse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "85741792-3de4-4145-90eb-f22efc92da1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS trip_db;\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "94e7d65c-6e3d-4f05-b567-a0b77994f076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "trip.write.mode(\"overwrite\").saveAsTable(\"trip_db.trip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "99387fbb-709d-4276-be6b-02cb23ddf23e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='trip', catalog='spark_catalog', namespace=['trip_db'], description=None, tableType='MANAGED', isTemporary=False),\n",
       " Table(name='trip_tmp_view', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables('trip_db')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0ab828ad-44e4-4ef2-9f3c-314e952af18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|VendorID|tpep_pickup_datetime|tpep_dropoff_datetime|passenger_count|trip_distance|PULocationID|DOLocationID|fare_amount|extra|tip_amount|total_amount|airport_fee|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "|       2|    02-01-2022 20:44|     02-01-2022 21:08|              1|         5.74|         107|           7|       20.5|  0.5|      4.86|       29.16|        0.0|\n",
      "|       1|    02-01-2022 20:35|     02-01-2022 20:44|              1|          1.3|         230|         229|        7.0|  3.0|       0.0|        10.8|        0.0|\n",
      "|       2|    02-01-2022 20:11|     02-01-2022 20:33|              1|         4.37|          79|         236|       18.0|  0.5|      4.36|       26.16|        0.0|\n",
      "|       2|    02-01-2022 20:49|     02-01-2022 20:52|              1|         0.46|         162|         229|        4.0|  0.5|       5.0|        12.8|        0.0|\n",
      "|       1|    02-01-2022 20:33|     02-01-2022 20:37|              1|          0.6|         211|         113|        4.5|  3.0|      1.65|        9.95|        0.0|\n",
      "+--------+--------------------+---------------------+---------------+-------------+------------+------------+-----------+-----+----------+------------+-----------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"SELECT * FROM trip_db.trip\")\n",
    "df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "adb56e08-66b7-4a87-964b-ccda7591db1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATA EXPLORATION WITH SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cde70dec-8f30-41ed-8aa9-5cc53130cf7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(trip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "59c4050f-bdce-4bf8-911a-ab601278f89f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+---------+-------+\n",
      "|            col_name|data_type|comment|\n",
      "+--------------------+---------+-------+\n",
      "|            VendorID|      int|   NULL|\n",
      "|tpep_pickup_datetime|   string|   NULL|\n",
      "|tpep_dropoff_date...|   string|   NULL|\n",
      "|     passenger_count|      int|   NULL|\n",
      "|       trip_distance|   double|   NULL|\n",
      "|        PULocationID|      int|   NULL|\n",
      "|        DOLocationID|      int|   NULL|\n",
      "|         fare_amount|   double|   NULL|\n",
      "|               extra|   double|   NULL|\n",
      "|          tip_amount|   double|   NULL|\n",
      "|        total_amount|   double|   NULL|\n",
      "|         airport_fee|   double|   NULL|\n",
      "+--------------------+---------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.sql(\"describe trip_db.trip\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "425ef384-efcc-4f92-815a-3fe17d2e3ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Data EXPLORATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "15f0c016-592c-4949-bde2-4716905fee01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|vendorID1|\n",
      "+---------+\n",
      "|   319007|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Run the SQL query to find the vendor 1 trips\n",
    "vendor1_df = spark.sql(\"SELECT COUNT(VendorID) as vendorID1 FROM trip_db.trip WHERE VendorID=1\")\n",
    "vendor1_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0aa32d60-dd02-4d92-9f86-5be7ba428222",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|vendorID2|\n",
      "+---------+\n",
      "|   724578|\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vendor2_df = spark.sql(\"SELECT COUNT(VendorID) as vendorID2 FROM trip_db.trip WHERE VendorID=2\")\n",
    "vendor2_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8beb825b-f247-4f81-85ff-0dca1b0e05df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+----------+\n",
      "|VendorID|fare_amount|tip_amount|\n",
      "+--------+-----------+----------+\n",
      "|       2|       52.0|    310.32|\n",
      "|       2|        5.0|    151.25|\n",
      "|       2|      130.0|     132.0|\n",
      "|       2|       97.0|     125.0|\n",
      "|       2|        8.5|     125.0|\n",
      "|       2|       14.0|    111.11|\n",
      "|       2|        2.5|     106.0|\n",
      "|       2|      510.0|    102.06|\n",
      "|       2|       14.5|     101.0|\n",
      "|       2|        2.5|     100.0|\n",
      "+--------+-----------+----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Find the top 10 trips with highest tip amounts\n",
    "\n",
    "top_10_trips_df=spark.sql(\"\"\"\n",
    "SELECT VendorID, fare_amount, tip_amount FROM (\n",
    "SELECT *,ROW_NUMBER() OVER (Order by tip_amount DESC) as row_num from trip_db.trip)\n",
    "where row_num<=10\"\"\")\n",
    "\n",
    "top_10_trips_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a486c7b8-9e42-4c41-b192-d283ab07ff5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+\n",
      "|DOLocationID|dropoff_count|\n",
      "+------------+-------------+\n",
      "|         236|        54140|\n",
      "+------------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the most popular drop-off location\n",
    "most_popular_dropoff_location = spark.sql(\"\"\"\n",
    "    SELECT DOLocationID, COUNT(*) AS dropoff_count\n",
    "    FROM trip_db.trip\n",
    "    GROUP BY DOLocationID\n",
    "    ORDER BY dropoff_count DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "most_popular_dropoff_location.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "76e717d2-59fe-4ed4-9702-ec8412e5c9b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+------------+\n",
      "|PULocationID|pickup_count|\n",
      "+------------+------------+\n",
      "|         237|       55209|\n",
      "+------------+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Find the most popular drop-off location\n",
    "most_popular_pickup_location = spark.sql(\"\"\"\n",
    "    SELECT PULocationID, COUNT(*) AS pickup_count\n",
    "    FROM trip_db.trip\n",
    "    GROUP BY PULocationID\n",
    "    ORDER BY pickup_count DESC\n",
    "    LIMIT 1\n",
    "\"\"\")\n",
    "\n",
    "most_popular_pickup_location.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf878e9-8c54-4390-8077-18b685bf5f1d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84444499-fbef-43c9-ab00-b545d33071c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "dea49ca8-e16e-4698-9a8a-b51a0678e931",
   "metadata": {},
   "source": [
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "# Assuming your DataFrame is named 'df'\n",
    "df = df.withColumn(\"tpep_pickup_datetime\", to_timestamp(df[\"tpep_pickup_datetime\"], \"MM-dd-yyyy HH:mm\"))\n",
    "df = df.withColumn(\"tpep_dropoff_datetime\", to_timestamp(df[\"tpep_dropoff_datetime\"], \"MM-dd-yyyy HH:mm\"))\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be9d5a50-0931-4304-a702-c86d5234296b",
   "metadata": {},
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "54c88769-ebc3-42a2-8d87-70ea1baa2575",
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tpep_pickup_datetime` cannot be resolved. Did you mean one of the following? [`col_name`, `data_type`, `comment`].",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[38], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m hour, dayofweek\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Extract hour of the day from 'tpep_pickup_datetime' and 'tpep_dropoff_datetime'\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpickup_hour\u001b[39m\u001b[38;5;124m'\u001b[39m, hour(\u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtpep_pickup_datetime\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m))\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mwithColumn(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdropoff_hour\u001b[39m\u001b[38;5;124m'\u001b[39m, hour(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtpep_dropoff_datetime\u001b[39m\u001b[38;5;124m'\u001b[39m]))\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# Extract day of the week from 'tpep_pickup_datetime' and 'tpep_dropoff_datetime'\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/sql/dataframe.py:3074\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   3002\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Returns the column as a :class:`Column`.\u001b[39;00m\n\u001b[1;32m   3003\u001b[0m \n\u001b[1;32m   3004\u001b[0m \u001b[38;5;124;03m.. versionadded:: 1.3.0\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3071\u001b[0m \u001b[38;5;124;03m+---+----+\u001b[39;00m\n\u001b[1;32m   3072\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   3073\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m-> 3074\u001b[0m     jc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mitem\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3075\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Column(jc)\n\u001b[1;32m   3076\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(item, Column):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/py4j/java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1316\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1317\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1318\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1319\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[1;32m   1321\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[0;32m-> 1322\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1323\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1325\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[1;32m   1326\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(temp_arg, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_detach\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.10/site-packages/pyspark/errors/exceptions/captured.py:185\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[0;34m(*a, **kw)\u001b[0m\n\u001b[1;32m    181\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[1;32m    183\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[0;32m--> 185\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mAnalysisException\u001b[0m: [UNRESOLVED_COLUMN.WITH_SUGGESTION] A column or function parameter with name `tpep_pickup_datetime` cannot be resolved. Did you mean one of the following? [`col_name`, `data_type`, `comment`]."
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import hour, dayofweek\n",
    "\n",
    "# Extract hour of the day from 'tpep_pickup_datetime' and 'tpep_dropoff_datetime'\n",
    "df = df.withColumn('pickup_hour', hour(df['tpep_pickup_datetime']))\n",
    "df = df.withColumn('dropoff_hour', hour(df['tpep_dropoff_datetime']))\n",
    "\n",
    "# Extract day of the week from 'tpep_pickup_datetime' and 'tpep_dropoff_datetime'\n",
    "df = df.withColumn('pickup_day_of_week', dayofweek(df['tpep_pickup_datetime']))\n",
    "df = df.withColumn('dropoff_day_of_week', dayofweek(df['tpep_dropoff_datetime']))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b74341ba-46c9-4752-8d96-ebb26618d89b",
   "metadata": {},
   "source": [
    "# Model Training"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1548517e-19e9-41df-bce8-c4471d7991f6",
   "metadata": {},
   "source": [
    "from pyspark.sql.types import IntegerType,BooleanType,DateType, StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0caa93-85b0-4bc0-b975-ef1b8820364a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data,test_data=df.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30c55d99-cc59-4b0a-8fd6-6c7b8eb12565",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StringIndexer\n",
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    " \n",
    "tpep_pickup_datetime_indexer = StringIndexer(inputCol='tpep_pickup_datetime',outputCol='tpep_pickup_datetime_index',handleInvalid='keep')\n",
    "tpep_dropoff_datetime_indexer = StringIndexer(inputCol='tpep_dropoff_datetime',outputCol='tpep_dropoff_datetime_index',handleInvalid='keep')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40c2bccc-058b-4dc2-8a14-ac3bfd13452c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b65b4ea-0d8d-4850-aef1-a2030c0ed05f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "# Vector assembler is used to create a vector of input features\n",
    " \n",
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'passenger_count',\n",
    "        'trip_distance',\n",
    "        'airport_fee',\n",
    "        'PULocationID',\n",
    "        'DOLocationID',\n",
    "        'tpep_dropoff_datetime_index',\n",
    "        'tpep_pickup_datetime_index'\n",
    "    ],\n",
    "    outputCol=\"features\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89eb16-1e0b-4619-9b42-2a7fcd086213",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline\n",
    "\n",
    "# Pipeline is used to pass the data through indexer and assembler simultaneously. Also, it helps to pre-rocess the test data\n",
    "# in the same way as that of the train data\n",
    "# https://spark.apache.org/docs/latest/ml-pipeline.html\n",
    " \n",
    "pipe = Pipeline(stages=[\n",
    "    tpep_dropoff_datetime_indexer,\n",
    "    tpep_pickup_datetime_indexer,\n",
    "    assembler\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3cb15d7-225c-430b-9d65-9a931e72bd3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "fitted_pipe=pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be48a194-1248-4058-a9ef-4a1b5d347fc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=fitted_pipe.transform(train_data)\n",
    "train_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9ce14b-d5f3-48c9-bd3e-be95a808e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=fitted_pipe.transform(test_data)\n",
    "test_data.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d921e8-4d16-4a96-aa30-eb4cb80bf999",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For those interested in utilizing the ML/AI power of Tensorflow with Spark....\n",
    "# https://github.com/tensorflow/ecosystem/tree/master/spark/spark-tensorflow-distributor\n",
    "\n",
    "# In this course, we'll use the SparkML (admitedely, it's not as powerful as Tensorflow, but \n",
    "# it's easy to use and demonstrate ML on a Spark Cluster)\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression\n",
    "\n",
    "lr_model = LinearRegression(labelCol='fare_amount')\n",
    "fit_model = lr_model.fit(train_data.select(['features','fare_amount']))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5bc99b7a-4e23-442b-ab45-a8db0e16dc46",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'fit_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mfit_model\u001b[49m\u001b[38;5;241m.\u001b[39mtransform(test_data)\n\u001b[1;32m      2\u001b[0m results\u001b[38;5;241m.\u001b[39mshow()\n",
      "\u001b[0;31mNameError\u001b[0m: name 'fit_model' is not defined"
     ]
    }
   ],
   "source": [
    "results = fit_model.transform(test_data)\n",
    "results.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "090f20b8-ee80-4a14-9cd9-63ec188ec069",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select(['fare_amount','prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18991818-9b72-4028-929d-2be8ae96ca07",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Evaluate performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b42ebec-34ca-4a2f-9ccb-faf9bb75b1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results = fit_model.evaluate(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca17849a-b081-4be0-aece-2405fedf6f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_results.residuals.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725744d0-4aa8-4c2d-baf7-8fa73f2f04f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'Ex Var:':7s} {test_results.explainedVariance:>7.3f}\")\n",
    "print(f\"{'MAE:':7s} {test_results.meanAbsoluteError:>7.3f}\")\n",
    "print(f\"{'MSE:':7s} {test_results.meanSquaredError:>7.3f}\")\n",
    "print(f\"{'RMSE:':7s} {test_results.rootMeanSquaredError:>7.3f}\")\n",
    "print(f\"{'R2:':7s} {test_results.r2:>7.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d7d57d-ef27-47cc-a7bc-9542ec9f15f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0890541-2b97-41d9-8c69-250659feb7fc",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b943f2-b663-4603-a6ad-a141a27d86bb",
   "metadata": {},
   "source": [
    "Whether a taxi trip results in a tip or not. Here's a modified version of your code for logistic regression:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "831ba887-0036-45be-a052-7ede5da2e9f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "from pyspark.ml.classification import LogisticRegression"
   ]
  },
  {
   "cell_type": "raw",
   "id": "91175c22-7155-4918-9fb1-b4bc9b9a8612",
   "metadata": {},
   "source": [
    "trip.printSchema()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b878917-de7c-4285-9fc0-1d37ed755bb6",
   "metadata": {},
   "source": [
    "# Define categorical and numeric columns\n",
    "categorical_columns = ['VendorID', 'PULocationID', 'DOLocationID']\n",
    "numeric_columns = ['passenger_count', 'trip_distance', 'extra', 'airport_fee']"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ec2f22f9-50f9-435c-b22c-ef9014bc5e71",
   "metadata": {},
   "source": [
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    "VendorID_indexer=StringIndexer(inputCol='VendorID', outputCol='VendorID_index',handleInvalid='keep')\n",
    "PULocationID_indexer=StringIndexer(inputCol='PULocationID', outputCol='PULocationID_index',handleInvalid='keep')\n",
    "DOLocationID_indexer=StringIndexer(inputCol='DOLocationID', outputCol='DOLocationID_index',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "46f76192-b476-49c9-99ac-4d2c233c6336",
   "metadata": {},
   "source": [
    "from pyspark.ml.feature import OneHotEncoder"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a414ac6-7cf5-4766-a19e-e8acc390ece6",
   "metadata": {},
   "source": [
    "data_encoder = OneHotEncoder(\n",
    "    inputCols=[\n",
    "        'VendorID_index',\n",
    "        'PULocationID_index',\n",
    "        'DOLocationID_index'\n",
    "    ], \n",
    "    outputCols= [\n",
    "        'VendorID_vec',\n",
    "        'PULocationID_vec',\n",
    "        'DOLocationID_vec'],\n",
    "    handleInvalid='keep'\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1024372b-534a-4516-bfe5-21e9fe97e6e7",
   "metadata": {},
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'VendorID_vec',\n",
    "        'PULocationID_vec',\n",
    "        'DOLocationID_vec','passenger_count', 'trip_distance', 'extra', 'airport_fee'\n",
    "        ],\n",
    "    outputCol=\"features_log\"\n",
    ")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "26b088ab-890e-4698-a35e-1dc16b58f64d",
   "metadata": {},
   "source": [
    "lr_model=LogisticRegression(labelCol='TIP_STATUS')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "585e37ee-2dc0-4961-9bd6-939dc71196ef",
   "metadata": {},
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        VendorID_indexer,\n",
    "    PULocationID_indexer,\n",
    "    DOLocationID_indexer,\n",
    "        data_encoder,\n",
    "        assembler,\n",
    "        lr_model\n",
    "    ]\n",
    ")\n",
    "  "
   ]
  },
  {
   "cell_type": "raw",
   "id": "3a477849-08dd-4a40-bd58-0fc24e5533bb",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "8c313506-1061-439f-9031-14b17b8605d5",
   "metadata": {},
   "source": [
    "# run the pipeline\n",
    "fit_model=pipe.fit(train_data)\n",
    "\n",
    "# Store the results in a dataframe\n",
    "results_log = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3800e3cd-44e2-4ea9-a02d-69040cf3638e",
   "metadata": {},
   "source": [
    "results_log.select(['TIP_STATUS','prediction']).show()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f9c7f68f-273b-47da-a8f4-99f424a6b897",
   "metadata": {},
   "source": [
    "# Model Evaluation"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8e6b32ab-4e8c-46b5-bfc0-87849b5965a7",
   "metadata": {},
   "source": [
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator\n",
    "\n",
    "AUC_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='TIP_STATUS',metricName='areaUnderROC')\n",
    "\n",
    "AUC = AUC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0528a56-47f8-468d-b817-037d6c4ba1a3",
   "metadata": {},
   "source": [
    "print(\"The area under the curve is {}\".format(AUC))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bfeb4015-df61-4580-b877-ba67427cb9d2",
   "metadata": {},
   "source": [
    "PR_evaluator = BinaryClassificationEvaluator(rawPredictionCol='prediction',labelCol='TIP_STATUS',metricName='areaUnderPR')\n",
    "PR = PR_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "be5f6e07-ea58-41d6-8f72-7e0f3d477430",
   "metadata": {},
   "source": [
    "print(\"The area under the PR curve is {}\".format(PR))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8f4de81-d2a4-4071-a049-1032df8119c3",
   "metadata": {},
   "source": [
    "## Accuracy"
   ]
  },
  {
   "cell_type": "raw",
   "id": "07bccd02-e584-4302-bd44-b9c970b8f7d6",
   "metadata": {},
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "\n",
    "ACC_evaluator = MulticlassClassificationEvaluator(  #  Multiclass or Binary, the accuracy is calculated in the same way.\n",
    "    labelCol=\"TIP_STATUS\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = ACC_evaluator.evaluate(results)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "bb07f9a1-e7f2-473b-976d-2b85ada899ad",
   "metadata": {},
   "source": [
    "print(\"The accuracy of the model is {}\".format(accuracy))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "8d6cd6f6-486c-4e5b-8c9b-99f0d029ef82",
   "metadata": {},
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "raw",
   "id": "593cb949-8b03-409f-8210-d41afa277435",
   "metadata": {},
   "source": [
    "y_true = results.select(\"TIP_STATUS\")\n",
    "y_true = y_true.toPandas()\n",
    " \n",
    "y_pred = results.select(\"prediction\")\n",
    "y_pred = y_pred.toPandas()\n",
    " \n",
    "cnf_matrix = confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3ee0ef6d-d87e-446d-8d7c-fdb49f26908b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "be7d010a-2b97-473d-bf27-36b12d3e7692",
   "metadata": {},
   "source": [
    "# DECISION TREE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e378997-dbfd-4aa6-a2eb-bc8b004b81aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import DecisionTreeClassifier\n",
    "from pyspark.ml.feature import VectorAssembler,StringIndexer\n",
    "from pyspark.ml import Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "198f3fb6-5d3f-4db0-8663-b61f03055030",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use StringIndexer to convert the categorical columns to hold numerical data\n",
    "VendorID_indexer=StringIndexer(inputCol='VendorID', outputCol='VendorID_index',handleInvalid='keep')\n",
    "PULocationID_indexer=StringIndexer(inputCol='PULocationID', outputCol='PULocationID_index',handleInvalid='keep')\n",
    "DOLocationID_indexer=StringIndexer(inputCol='DOLocationID', outputCol='DOLocationID_index',handleInvalid='keep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f220c7f9-9702-42bb-970a-2c5023285817",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(\n",
    "    inputCols=[\n",
    "        'VendorID_index',\n",
    "        'PULocationID_index',\n",
    "        'DOLocationID_index',\n",
    "        'passenger_count', 'trip_distance', 'extra', 'airport_fee'\n",
    "    ],\n",
    "    outputCol=\"features_dtree\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cc03e6-7cb5-4e8b-9ca9-c5a0c5c99a31",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_model = DecisionTreeClassifier(labelCol='TIP_STATUS',maxBins=16000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdd9ba8-3a8f-469c-ada5-a765c1cb56a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe = Pipeline(\n",
    "    stages=[\n",
    "        VendorID_indexer,\n",
    "        PULocationID_indexer,\n",
    "        DOLocationID_indexer,\n",
    "        assembler,\n",
    "        dt_model\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9230da82-7173-4e07-a617-a2d1b867e7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_model=pipe.fit(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2689ea5-e36f-4f50-8527-9cea375bdded",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = fit_model.transform(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa78255a-dadb-440c-af78-18351cfba9e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.select(['TIP_STATUS','prediction']).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa752e84-03aa-499c-b9e2-2563e41fc93c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde5489b-a402-4f20-9ef2-bbe404aeef1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ACC_evaluator = MulticlassClassificationEvaluator(\n",
    "    labelCol=\"TIP_STATUS\", predictionCol=\"prediction\", metricName=\"accuracy\")\n",
    "\n",
    "accuracy = ACC_evaluator.evaluate(results)\n",
    "\n",
    "print(f\"The accuracy of the decision tree classifier is {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aa84263-fd8e-4790-b7a1-3a977218af58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
